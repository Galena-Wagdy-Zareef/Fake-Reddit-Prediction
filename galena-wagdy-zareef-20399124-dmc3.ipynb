{"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["xtuxWrmAvyuK","7opYAnLnap5E","inwm5qPoGMGh","2ZDj6MYdbJE8","FUNKxyseA-_E","DRQpCfV5vHV_","yeDxVFZa5zMT","gBKM7B8LDl4R","WfzDqPRcDtty","Nq8suBMAD5Qq","mw3_7Ecz5tYd","csmlO-3Q-lX0","Oe88Y-jkIQqN","ecp-D9bBIQqO","a44vd1yaIQqP","H3TH-yySIRPF","xZVlbwgtIRPH","O-9zt3wuIRPK","HxuFSYtBOE8-","fKETHVt3OE9W","YeQlhoajOE9X","IFnrhL-lOE9Z","Ax1UnBTbT_sp","-co-fl0xT_s6","y81hcIdXT_s6","LZ1tfqodT_s7","9OukgEy3T_s7","mSDw1W7vT_s7","mdd1r1xBT_s8","UurAtB4Oomlv","mO7olZ5loml1","CQeYfABXoml1","q6KgflBFoml2","N2kRfwlUsaT7","3qTokW05saT8","9H_eaFyfsaUA","_K-ST3qXsaUA","neVXPqefsaUB","uvEIQ-tUsaUC","Y6l6dHSnsaUD","MQLN3yZnsaUD","zJ_fY-2CsaUE","tx1nhMOT0lDP"],"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**cisc-873-dm-w23-a3\nFake Reddit Prediction**\nThe dataset contains a collection of news articles that are labeled as reliable or unreliable.\nThe problem is to identify unreliable news articles. The input is a news article and the output is a binary classification of reliable or unreliable. The data mining function required is classification.\nthe input is text column and the output classification.\nSome challenges include the fact that there are many different types of fake news and it can be difficult to identify them all. Additionally, some fake news articles may be intentionally misleading and difficult to detect.\nThe impact of fake news can be significant as it can lead to misinformation and confusion among readers. An ideal solution would be a system that can accurately identify unreliable news articles and prevent them from being shared widely.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:06:35.301951Z","iopub.execute_input":"2023-04-06T00:06:35.302411Z","iopub.status.idle":"2023-04-06T00:06:35.349230Z","shell.execute_reply.started":"2023-04-06T00:06:35.302370Z","shell.execute_reply":"2023-04-06T00:06:35.348095Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cisc-873-dm-w23-a3/sample_submission.csv\n/kaggle/input/cisc-873-dm-w23-a3/x_test.csv\n/kaggle/input/cisc-873-dm-w23-a3/xy_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#import liberaries \nimport re\nimport pickle\nimport sklearn\nimport holoviews as hv \nimport nltk\nfrom bokeh.io import output_notebook\noutput_notebook()\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom xgboost import XGBClassifier\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# some seeting for pandas and hvplot\npd.options.display.max_columns = 100 # Determine the maximum number of columns that I want to appear when displaying the dataframe\npd.options.display.max_rows = 300 # Determine the maximum number of rows that I want to appear when displaying the dataframe\npd.options.display.max_colwidth = 100 # Maximum width of columns\nnp.set_printoptions(threshold=2000)","metadata":{"id":"rIg9u0bDoXt8","execution":{"iopub.status.busy":"2023-04-06T00:06:35.350935Z","iopub.execute_input":"2023-04-06T00:06:35.352157Z","iopub.status.idle":"2023-04-06T00:06:39.639360Z","shell.execute_reply.started":"2023-04-06T00:06:35.352112Z","shell.execute_reply":"2023-04-06T00:06:39.637809Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":"<div class=\"bk-root\">\n        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n        <span id=\"1002\">Loading BokehJS ...</span>\n    </div>\n"},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":"(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));","application/vnd.bokehjs_load.v0+json":"(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"},"metadata":{}}]},{"cell_type":"code","source":"# Read training data (.csv) and inistialize in data_tr variable to use it\ndata_tr = pd.read_csv('/kaggle/input/cisc-873-dm-w23-a3/xy_train.csv')\ndata_tr.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haSjfQKGaTAE","outputId":"c7d7a883-a3e9-4884-80e7-958c3c9721c0","execution":{"iopub.status.busy":"2023-04-06T00:06:39.642260Z","iopub.execute_input":"2023-04-06T00:06:39.643513Z","iopub.status.idle":"2023-04-06T00:06:39.955946Z","shell.execute_reply.started":"2023-04-06T00:06:39.643452Z","shell.execute_reply":"2023-04-06T00:06:39.954627Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       id  \\\n0  265723   \n1  284269   \n2  207715   \n3  551106   \n4    8584   \n\n                                                                                                  text  \\\n0  A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n1  British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n2  In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n3  Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n4  Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n\n   label  \n0      0  \n1      0  \n2      0  \n3      0  \n4      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>265723</td>\n      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>284269</td>\n      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>207715</td>\n      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>551106</td>\n      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8584</td>\n      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(type(data_tr))\nprint(data_tr.shape)\ndata_tr.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:06:39.958869Z","iopub.execute_input":"2023-04-06T00:06:39.959275Z","iopub.status.idle":"2023-04-06T00:06:39.974768Z","shell.execute_reply.started":"2023-04-06T00:06:39.959236Z","shell.execute_reply":"2023-04-06T00:06:39.973218Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\n(60000, 3)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       id  \\\n0  265723   \n1  284269   \n2  207715   \n3  551106   \n4    8584   \n\n                                                                                                  text  \\\n0  A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n1  British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n2  In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n3  Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n4  Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n\n   label  \n0      0  \n1      0  \n2      0  \n3      0  \n4      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>265723</td>\n      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>284269</td>\n      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>207715</td>\n      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>551106</td>\n      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8584</td>\n      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Read testing data (.csv) and inistialize in data_ts variable to use it\ndata_ts = pd.read_csv('/kaggle/input/cisc-873-dm-w23-a3/x_test.csv')\ndata_ts.head(5)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1uSmq39rabjY","outputId":"f9923308-f300-4e5f-92d1-f01bad8edb29","execution":{"iopub.status.busy":"2023-04-06T00:06:39.976417Z","iopub.execute_input":"2023-04-06T00:06:39.976831Z","iopub.status.idle":"2023-04-06T00:06:40.140843Z","shell.execute_reply.started":"2023-04-06T00:06:39.976791Z","shell.execute_reply":"2023-04-06T00:06:40.139639Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   id                                                            text\n0   0                                                      stargazer \n1   1                                                            yeah\n2   2      PD: Phoenix car thief gets instructions from YouTube video\n3   3  As Trump Accuses Iran, He Has One Problem: His Own Credibility\n4   4                                    \"Believers\" - Hezbollah 2011","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>stargazer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>PD: Phoenix car thief gets instructions from YouTube video</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>As Trump Accuses Iran, He Has One Problem: His Own Credibility</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>\"Believers\" - Hezbollah 2011</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(type(data_ts))\nprint(data_ts.shape)\ndata_ts.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:06:40.142408Z","iopub.execute_input":"2023-04-06T00:06:40.142759Z","iopub.status.idle":"2023-04-06T00:06:40.155662Z","shell.execute_reply.started":"2023-04-06T00:06:40.142726Z","shell.execute_reply":"2023-04-06T00:06:40.154219Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\n(59151, 2)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   id                                                            text\n0   0                                                      stargazer \n1   1                                                            yeah\n2   2      PD: Phoenix car thief gets instructions from YouTube video\n3   3  As Trump Accuses Iran, He Has One Problem: His Own Credibility\n4   4                                    \"Believers\" - Hezbollah 2011","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>stargazer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>PD: Phoenix car thief gets instructions from YouTube video</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>As Trump Accuses Iran, He Has One Problem: His Own Credibility</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>\"Believers\" - Hezbollah 2011</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Display the column's name in training and testing data\nprint(data_tr.columns)\nprint(data_ts.columns)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xl_Iq1A48o7y","outputId":"2e021e0d-1d27-4ff6-b352-982ff6fe8b76","execution":{"iopub.status.busy":"2023-04-06T00:06:40.157349Z","iopub.execute_input":"2023-04-06T00:06:40.158450Z","iopub.status.idle":"2023-04-06T00:06:40.164687Z","shell.execute_reply.started":"2023-04-06T00:06:40.158410Z","shell.execute_reply":"2023-04-06T00:06:40.163507Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Index(['id', 'text', 'label'], dtype='object')\nIndex(['id', 'text'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display the dataframe's information in training and testing data\nprint(data_tr.info())\nprint(data_ts.info())","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:06:40.166455Z","iopub.execute_input":"2023-04-06T00:06:40.166839Z","iopub.status.idle":"2023-04-06T00:06:40.212220Z","shell.execute_reply.started":"2023-04-06T00:06:40.166802Z","shell.execute_reply":"2023-04-06T00:06:40.210908Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60000 entries, 0 to 59999\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      60000 non-null  int64 \n 1   text    60000 non-null  object\n 2   label   60000 non-null  int64 \ndtypes: int64(2), object(1)\nmemory usage: 1.4+ MB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 59151 entries, 0 to 59150\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      59151 non-null  int64 \n 1   text    59151 non-null  object\ndtypes: int64(1), object(1)\nmemory usage: 924.4+ KB\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\n\nstemmer = SnowballStemmer(\"english\")   # It is the method used to return the word to its original form\nstop_words = set(stopwords.words(\"english\")) # It is the method of producing a stop words\n\ndef clean_text(text):\n    \"\"\" steps:\n        - remove any html tags (< /br> often found)\n        - Keep only ASCII + European Chars and whitespace, no digits\n        - remove single letter chars\n        - convert all whitespaces (tabs etc.) to single wspace\n        if not for embedding (but e.g. tdf-idf):\n        - all lowercase\n        - remove stopwords, punctuation and stemm\n    \"\"\"\n    # IGNORECASE : is a flag allows for case-insensitive matching of the Regular Expression with the given string\n    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE) # Remove any more than one space\n    RE_TAGS = re.compile(r\"<[^>]+>\") # Remove web tags\n    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE) # Remove any leter does not english charachter\n    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE) # Remove any single character\n\n    text = re.sub(RE_TAGS, \" \", text)# Replace any tag with a single space.\n    text = re.sub(RE_ASCII, \" \", text) # Replace any non english character with a single space.\n    text = re.sub(RE_SINGLECHAR, \" \", text) # Replace any single character with a single space.\n    text = re.sub(RE_WSPACE, \" \", text)  # Replace any more than one space with a single space.\n\n    word_tokens = word_tokenize(text) # split the sentence into words\n    words_tokens_lower = [word.lower() for word in word_tokens] # Convert all letters to small letters\n\n    # words_filtered (Words can be filtered based on how many times they appear)\n    # stemmer used to return the word to its original form.\n    words_filtered = [\n        stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n    ]\n\n    # Join all words in text_clean and separate them by space.\n    text_clean = \" \".join(words_filtered)\n    return text_clean","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ug1LefLKqfL7","outputId":"541d204a-78ec-42d0-eef0-e24adc1a729e","execution":{"iopub.status.busy":"2023-04-06T00:06:40.214180Z","iopub.execute_input":"2023-04-06T00:06:40.214557Z","iopub.status.idle":"2023-04-06T00:06:40.392286Z","shell.execute_reply.started":"2023-04-06T00:06:40.214521Z","shell.execute_reply":"2023-04-06T00:06:40.391283Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# call clean_text function that take string as a parameter to test the function\nclean_text(\"Python is a basic programming language .\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0ti4Eh8Nr_4c","outputId":"b55421ac-27c3-40d9-b796-69405a50c7b1","execution":{"iopub.status.busy":"2023-04-06T00:06:40.397646Z","iopub.execute_input":"2023-04-06T00:06:40.398092Z","iopub.status.idle":"2023-04-06T00:06:40.419468Z","shell.execute_reply.started":"2023-04-06T00:06:40.398046Z","shell.execute_reply":"2023-04-06T00:06:40.417988Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'python basic program languag'"},"metadata":{}}]},{"cell_type":"code","source":"# Clean texts by taking any text that's length is greater than 25\n# map is an iterator function that returns a result after applying a clean_text function to each item in an iterable \n# lambda is a function used to apply certain functions to all rows of a data set.\n# lambda take one argument (x) then put x in clean_text function\n# if statement means ( if input x is string enter x to clean_text function then the result put in data['clean_com'] if not return x in data['clean_com'] as it is  )\ndata_tr['clean_text']=data_tr.loc[data_tr['text'].str.len()>25,\"text\"]\ndata_tr['clean_text']=data_tr['clean_text'].map(\n    lambda x: clean_text(x) if isinstance(x, str) else x   \n)\n\ndata_ts['clean_text']=data_ts.loc[data_ts['text'].str.len()>0,\"text\"]\ndata_ts['clean_text']=data_ts['clean_text'].map(\n    lambda x: clean_text(x) if isinstance(x, str) else x   \n)\n\n","metadata":{"id":"kgp6f477r_5Y","execution":{"iopub.status.busy":"2023-04-06T00:06:40.420838Z","iopub.execute_input":"2023-04-06T00:06:40.421245Z","iopub.status.idle":"2023-04-06T00:07:18.592870Z","shell.execute_reply.started":"2023-04-06T00:06:40.421206Z","shell.execute_reply":"2023-04-06T00:07:18.591440Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_tr","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:07:18.594640Z","iopub.execute_input":"2023-04-06T00:07:18.595187Z","iopub.status.idle":"2023-04-06T00:07:18.630626Z","shell.execute_reply.started":"2023-04-06T00:07:18.595133Z","shell.execute_reply":"2023-04-06T00:07:18.629214Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"           id  \\\n0      265723   \n1      284269   \n2      207715   \n3      551106   \n4        8584   \n...       ...   \n59995   70046   \n59996  189377   \n59997   93486   \n59998  140950   \n59999   34509   \n\n                                                                                                      text  \\\n0      A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n1      British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n2      In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n3      Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n4      Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n...                                                                                                    ...   \n59995                Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)   \n59996                Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back   \n59997                Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no   \n59998                Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)   \n59999                Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep   \n\n       label  \\\n0          0   \n1          0   \n2          0   \n3          0   \n4          0   \n...      ...   \n59995      0   \n59996      1   \n59997      0   \n59998      0   \n59999      1   \n\n                                                                                                clean_text  \n0      group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...  \n1      british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...  \n2      goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...  \n3      happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...  \n4      obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...  \n...                                                                                                    ...  \n59995                                                       finish sniper simo yh invas finland ussr color  \n59996                                               nigerian princ scam took kansa man year later get back  \n59997                                                         safe smoke marijuana pregnanc surpris answer  \n59998                                               julius caesar upon realiz everyon room knife except bc  \n59999                                        jeff bridg releas leep tape new album design help fall asleep  \n\n[60000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>265723</td>\n      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n      <td>0</td>\n      <td>group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>284269</td>\n      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n      <td>0</td>\n      <td>british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>207715</td>\n      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n      <td>0</td>\n      <td>goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>551106</td>\n      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n      <td>0</td>\n      <td>happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8584</td>\n      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n      <td>0</td>\n      <td>obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>70046</td>\n      <td>Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)</td>\n      <td>0</td>\n      <td>finish sniper simo yh invas finland ussr color</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>189377</td>\n      <td>Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back</td>\n      <td>1</td>\n      <td>nigerian princ scam took kansa man year later get back</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>93486</td>\n      <td>Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no</td>\n      <td>0</td>\n      <td>safe smoke marijuana pregnanc surpris answer</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>140950</td>\n      <td>Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)</td>\n      <td>0</td>\n      <td>julius caesar upon realiz everyon room knife except bc</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>34509</td>\n      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep</td>\n      <td>1</td>\n      <td>jeff bridg releas leep tape new album design help fall asleep</td>\n    </tr>\n  </tbody>\n</table>\n<p>60000 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_ts","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"GVm5gux3r_6H","outputId":"08f0ec13-7f17-4c7c-be42-644fa2899631","execution":{"iopub.status.busy":"2023-04-06T00:07:18.632023Z","iopub.execute_input":"2023-04-06T00:07:18.632551Z","iopub.status.idle":"2023-04-06T00:07:18.654476Z","shell.execute_reply.started":"2023-04-06T00:07:18.632493Z","shell.execute_reply":"2023-04-06T00:07:18.653211Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"          id  \\\n0          0   \n1          1   \n2          2   \n3          3   \n4          4   \n...      ...   \n59146  59146   \n59147  59147   \n59148  59148   \n59149  59149   \n59150  59150   \n\n                                                                                  text  \\\n0                                                                           stargazer    \n1                                                                                 yeah   \n2                           PD: Phoenix car thief gets instructions from YouTube video   \n3                       As Trump Accuses Iran, He Has One Problem: His Own Credibility   \n4                                                         \"Believers\" - Hezbollah 2011   \n...                                                                                ...   \n59146                                                Bicycle taxi drivers of New Delhi   \n59147                             Trump blows up GOP's formula for winning House races   \n59148  Napoleon returns from his exile on the island of Elba. (March 1815), Colourised   \n59149                                 Deep down he always wanted to be a ballet dancer   \n59150                        Toddler miraculously survives 6-story fall landing on car   \n\n                                            clean_text  \n0                                              stargaz  \n1                                                 yeah  \n2       pd phoenix car thief get instruct youtub video  \n3                 trump accus iran one problem credibl  \n4                                     believ hezbollah  \n...                                                ...  \n59146                     bicycl taxi driver new delhi  \n59147             trump blow gop formula win hous race  \n59148  napoleon return exil island elba march colouris  \n59149                    deep alway want ballet dancer  \n59150       toddler miracul surviv stori fall land car  \n\n[59151 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>stargazer</td>\n      <td>stargaz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>yeah</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>PD: Phoenix car thief gets instructions from YouTube video</td>\n      <td>pd phoenix car thief get instruct youtub video</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>As Trump Accuses Iran, He Has One Problem: His Own Credibility</td>\n      <td>trump accus iran one problem credibl</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>\"Believers\" - Hezbollah 2011</td>\n      <td>believ hezbollah</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59146</th>\n      <td>59146</td>\n      <td>Bicycle taxi drivers of New Delhi</td>\n      <td>bicycl taxi driver new delhi</td>\n    </tr>\n    <tr>\n      <th>59147</th>\n      <td>59147</td>\n      <td>Trump blows up GOP's formula for winning House races</td>\n      <td>trump blow gop formula win hous race</td>\n    </tr>\n    <tr>\n      <th>59148</th>\n      <td>59148</td>\n      <td>Napoleon returns from his exile on the island of Elba. (March 1815), Colourised</td>\n      <td>napoleon return exil island elba march colouris</td>\n    </tr>\n    <tr>\n      <th>59149</th>\n      <td>59149</td>\n      <td>Deep down he always wanted to be a ballet dancer</td>\n      <td>deep alway want ballet dancer</td>\n    </tr>\n    <tr>\n      <th>59150</th>\n      <td>59150</td>\n      <td>Toddler miraculously survives 6-story fall landing on car</td>\n      <td>toddler miracul surviv stori fall land car</td>\n    </tr>\n  </tbody>\n</table>\n<p>59151 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Examine the label column for unique values and the number of times they appear.\ndata_tr['label'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umamQkkEr_66","outputId":"309a3994-f039-401c-c5c7-14740ee051dd","execution":{"iopub.status.busy":"2023-04-06T00:07:18.656362Z","iopub.execute_input":"2023-04-06T00:07:18.657402Z","iopub.status.idle":"2023-04-06T00:07:18.672107Z","shell.execute_reply.started":"2023-04-06T00:07:18.657346Z","shell.execute_reply":"2023-04-06T00:07:18.670143Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0    32172\n1    27596\n2      232\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# drop rows that has label = 2 \ndrop2=data_tr['label']==2\ndata_tr.drop(data_tr.index[drop2],inplace=True)","metadata":{"id":"wGKgToSEsXUN","execution":{"iopub.status.busy":"2023-04-06T00:07:18.674392Z","iopub.execute_input":"2023-04-06T00:07:18.675372Z","iopub.status.idle":"2023-04-06T00:07:18.704179Z","shell.execute_reply.started":"2023-04-06T00:07:18.675314Z","shell.execute_reply":"2023-04-06T00:07:18.701529Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Examine the label column for unique values and the number of times they appear.\ndata_tr['label'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcfYrrOksXVD","outputId":"2fce72ff-ed0c-4bba-b716-a9e2cc564871","execution":{"iopub.status.busy":"2023-04-06T00:07:18.706213Z","iopub.execute_input":"2023-04-06T00:07:18.706739Z","iopub.status.idle":"2023-04-06T00:07:18.720440Z","shell.execute_reply.started":"2023-04-06T00:07:18.706684Z","shell.execute_reply":"2023-04-06T00:07:18.718974Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0    32172\n1    27596\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# copy data in data_clean\ndata_tr_clean=data_tr.copy()","metadata":{"id":"IcBpDySOsXV5","execution":{"iopub.status.busy":"2023-04-06T00:07:18.722690Z","iopub.execute_input":"2023-04-06T00:07:18.723311Z","iopub.status.idle":"2023-04-06T00:07:18.732269Z","shell.execute_reply.started":"2023-04-06T00:07:18.723266Z","shell.execute_reply":"2023-04-06T00:07:18.730988Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data_tr_clean.head(5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"0K09Rm5bsXWl","outputId":"de615b56-3078-4b2b-fb0d-63ee76b75bd0","execution":{"iopub.status.busy":"2023-04-06T00:07:18.734566Z","iopub.execute_input":"2023-04-06T00:07:18.735700Z","iopub.status.idle":"2023-04-06T00:07:18.752259Z","shell.execute_reply.started":"2023-04-06T00:07:18.735644Z","shell.execute_reply":"2023-04-06T00:07:18.750733Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"       id  \\\n0  265723   \n1  284269   \n2  207715   \n3  551106   \n4    8584   \n\n                                                                                                  text  \\\n0  A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n1  British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n2  In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n3  Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n4  Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n\n   label  \\\n0      0   \n1      0   \n2      0   \n3      0   \n4      0   \n\n                                                                                            clean_text  \n0  group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...  \n1  british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...  \n2  goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...  \n3  happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...  \n4  obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>265723</td>\n      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n      <td>0</td>\n      <td>group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>284269</td>\n      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n      <td>0</td>\n      <td>british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>207715</td>\n      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n      <td>0</td>\n      <td>goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>551106</td>\n      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n      <td>0</td>\n      <td>happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8584</td>\n      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n      <td>0</td>\n      <td>obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Word Frequency of most common words\n# Split all words with whitespace between them, then put them in word_freq.\nword_freq_tr = pd.Series(\" \".join(data_tr_clean[\"clean_text\"]).split()).value_counts()\nword_freq_tr[1:40] # display the first 40 words with frequencies","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUC_fJXbsXd4","outputId":"08358db1-6b18-4899-c492-37e5fe6c3dbd","execution":{"iopub.status.busy":"2023-04-06T00:07:18.753988Z","iopub.execute_input":"2023-04-06T00:07:18.754597Z","iopub.status.idle":"2023-04-06T00:07:19.054307Z","shell.execute_reply.started":"2023-04-06T00:07:18.754558Z","shell.execute_reply":"2023-04-06T00:07:19.052877Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"one         3285\nlike        3128\nnew         2998\nlook        2847\ncolor       2737\nman         2729\nget         2602\ntrump       2578\nsay         2347\npeopl       2316\nuse         2307\nfirst       2248\nmake        2227\nold         2226\ntime        2027\nposter      2000\nfound       1999\nday         1935\nwar         1858\npost        1648\nworld       1570\nwork        1531\nshow        1513\nus          1506\namerican    1504\ntake        1491\nlife        1482\npsbattl     1470\nhelp        1442\ngo          1420\nstate       1409\nback        1369\ntwo         1364\nschool      1345\nsee         1329\nphoto       1324\nmade        1314\nright       1311\nsave        1308\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# display list most uncommon words  \n# reset_index is reset the index of the DataFrame\nword_freq_tr[-10:].reset_index(name=\"freq\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"vopmkeLCsXgX","outputId":"0a418914-d9e9-4569-860d-486e9cd35aa8","execution":{"iopub.status.busy":"2023-04-06T00:07:19.055787Z","iopub.execute_input":"2023-04-06T00:07:19.056232Z","iopub.status.idle":"2023-04-06T00:07:19.070872Z","shell.execute_reply.started":"2023-04-06T00:07:19.056191Z","shell.execute_reply":"2023-04-06T00:07:19.069535Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"      index  freq\n0   angriff     1\n1  delusion     1\n2      wane     1\n3  undament     1\n4      miku     1\n5    hatsun     1\n6     nfler     1\n7    hicock     1\n8    mccall     1\n9      wahr     1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>freq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>angriff</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>delusion</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wane</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>undament</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>miku</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>hatsun</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>nfler</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>hicock</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>mccall</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>wahr</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Distribution of ratings\ndata_tr_clean[\"label\"].value_counts(normalize=True) # count proportions of label","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AaDrTiC_s7YS","outputId":"385bc4dc-fce4-4c25-ce19-0bd991c3f697","execution":{"iopub.status.busy":"2023-04-06T00:07:19.072295Z","iopub.execute_input":"2023-04-06T00:07:19.072645Z","iopub.status.idle":"2023-04-06T00:07:19.091742Z","shell.execute_reply.started":"2023-04-06T00:07:19.072611Z","shell.execute_reply":"2023-04-06T00:07:19.090039Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"0    0.538281\n1    0.461719\nName: label, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\nCompute unique word vector with frequencies\nexclude very uncommon (<10 obsv.) and common (>=30%) words\nuse pairs of two words (ngram)\n\"\"\"\n# TfidfVectorizer convert a collection of raw documents to a matrix of TF-IDF features.\n# min_df ignore terms that have a document frequency strictly lower than the given threshold\n# max_df ignore terms that have a document frequency strictly higher than the given threshold\n# ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n# ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n\nvectorizer = TfidfVectorizer(\n    max_df=0.3, min_df=10, ngram_range=(1, 2)\n)\nvectorizer.fit(data_tr_clean[\"clean_text\"])\n\n# vectorizer = TfidfVectorizer(\n#     analyzer=\"word\", max_df=0.3, min_df=10, ngram_range=(1, 2), norm=\"l2\"\n# )\n# vectorizer.fit(data_clean[\"clean_text\"])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y06v9PRps7au","outputId":"bb8cd843-724f-43cc-a4fd-3973f4fb9a2a","execution":{"iopub.status.busy":"2023-04-06T00:07:19.093679Z","iopub.execute_input":"2023-04-06T00:07:19.094076Z","iopub.status.idle":"2023-04-06T00:07:21.615686Z","shell.execute_reply.started":"2023-04-06T00:07:19.094037Z","shell.execute_reply":"2023-04-06T00:07:21.614357Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TfidfVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 2))"},"metadata":{}}]},{"cell_type":"code","source":"# Vector representation of vocabulary show some sample from our data_clean\nword_vector = pd.Series(vectorizer.vocabulary_).sample(5, random_state=1) # By sample, I choose the number of vocabulary that I want to display.\nprint(f\"Unique word (ngram) vector extract:\\n\\n {word_vector}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37qyZ2dRs7de","outputId":"12f5a7d8-c4a2-4e9a-ce2c-c78590c49c48","execution":{"iopub.status.busy":"2023-04-06T00:07:21.617636Z","iopub.execute_input":"2023-04-06T00:07:21.618270Z","iopub.status.idle":"2023-04-06T00:07:21.637252Z","shell.execute_reply.started":"2023-04-06T00:07:21.618218Z","shell.execute_reply":"2023-04-06T00:07:21.635821Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Unique word (ngram) vector extract:\n\n eli       2666\ngo far    3595\nbamboo     650\nwisdom    9837\npocket    6705\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"data_tr_clean","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"5DwjEU01s7f-","outputId":"583de9d0-e95f-4eaa-c380-79c00c28925f","execution":{"iopub.status.busy":"2023-04-06T00:07:21.638523Z","iopub.execute_input":"2023-04-06T00:07:21.639415Z","iopub.status.idle":"2023-04-06T00:07:21.660990Z","shell.execute_reply.started":"2023-04-06T00:07:21.639376Z","shell.execute_reply":"2023-04-06T00:07:21.659728Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"           id  \\\n0      265723   \n1      284269   \n2      207715   \n3      551106   \n4        8584   \n...       ...   \n59995   70046   \n59996  189377   \n59997   93486   \n59998  140950   \n59999   34509   \n\n                                                                                                      text  \\\n0      A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n1      British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n2      In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n3      Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n4      Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n...                                                                                                    ...   \n59995                Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)   \n59996                Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back   \n59997                Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no   \n59998                Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)   \n59999                Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep   \n\n       label  \\\n0          0   \n1          0   \n2          0   \n3          0   \n4          0   \n...      ...   \n59995      0   \n59996      1   \n59997      0   \n59998      0   \n59999      1   \n\n                                                                                                clean_text  \n0      group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...  \n1      british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...  \n2      goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...  \n3      happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...  \n4      obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...  \n...                                                                                                    ...  \n59995                                                       finish sniper simo yh invas finland ussr color  \n59996                                               nigerian princ scam took kansa man year later get back  \n59997                                                         safe smoke marijuana pregnanc surpris answer  \n59998                                               julius caesar upon realiz everyon room knife except bc  \n59999                                        jeff bridg releas leep tape new album design help fall asleep  \n\n[59768 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>265723</td>\n      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n      <td>0</td>\n      <td>group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>284269</td>\n      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n      <td>0</td>\n      <td>british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>207715</td>\n      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n      <td>0</td>\n      <td>goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>551106</td>\n      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n      <td>0</td>\n      <td>happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8584</td>\n      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n      <td>0</td>\n      <td>obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>70046</td>\n      <td>Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)</td>\n      <td>0</td>\n      <td>finish sniper simo yh invas finland ussr color</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>189377</td>\n      <td>Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back</td>\n      <td>1</td>\n      <td>nigerian princ scam took kansa man year later get back</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>93486</td>\n      <td>Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no</td>\n      <td>0</td>\n      <td>safe smoke marijuana pregnanc surpris answer</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>140950</td>\n      <td>Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)</td>\n      <td>0</td>\n      <td>julius caesar upon realiz everyon room knife except bc</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>34509</td>\n      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep</td>\n      <td>1</td>\n      <td>jeff bridg releas leep tape new album design help fall asleep</td>\n    </tr>\n  </tbody>\n</table>\n<p>59768 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# splitting Trainig data into X_train and y_train\nX=data_tr_clean['clean_text'] # X contains only clean_text column\ny=data_tr_clean['label']   # y contains only label column\nX_test=data_ts['clean_text'] # X_test contains only clean_text column","metadata":{"id":"K6jo-tJ7s7iW","execution":{"iopub.status.busy":"2023-04-06T00:07:21.662809Z","iopub.execute_input":"2023-04-06T00:07:21.663201Z","iopub.status.idle":"2023-04-06T00:07:21.669447Z","shell.execute_reply.started":"2023-04-06T00:07:21.663165Z","shell.execute_reply":"2023-04-06T00:07:21.667979Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# transform each sentence to numeric vector with tf-idf value as elements\nX_train_vec = vectorizer.transform(X)\nX_test_vec = vectorizer.transform(X_test)\nX_train_vec.get_shape()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmSW5D2as7lJ","outputId":"a487c4b9-8ea0-4f0a-96ff-2f8eebacfac4","execution":{"iopub.status.busy":"2023-04-06T00:07:21.671169Z","iopub.execute_input":"2023-04-06T00:07:21.671512Z","iopub.status.idle":"2023-04-06T00:07:24.028203Z","shell.execute_reply.started":"2023-04-06T00:07:21.671480Z","shell.execute_reply":"2023-04-06T00:07:24.026757Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(59768, 10061)"},"metadata":{}}]},{"cell_type":"code","source":"# Compare original comment text with its numeric vector representation\nprint(f\"Original sentence:\\n{X[3:4].values}\\n\")\n# Feature is a dataframe that takes X_train_vec and converts it to an array, and the column is the name of the feature\nfeatures = pd.DataFrame(\n    X_train_vec[3:4].toarray(), columns=vectorizer.get_feature_names()\n)\nnonempty_feat = features.loc[:, (features != 0).any(axis=0)]\nprint(f\"Vector representation of sentence:\\n {nonempty_feat}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8_O3uEvvU5t","outputId":"59457c56-12ac-42e2-c66f-322f0f68b1d5","execution":{"iopub.status.busy":"2023-04-06T00:07:24.030159Z","iopub.execute_input":"2023-04-06T00:07:24.030528Z","iopub.status.idle":"2023-04-06T00:07:24.110335Z","shell.execute_reply.started":"2023-04-06T00:07:24.030492Z","shell.execute_reply":"2023-04-06T00:07:24.109061Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Original sentence:\n['happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorporateshil irish rover jolli rove tar imgur com true dh bqw https extern preview redd qseokdslwivxzdehgxxbceuxh vhemi muoxw yaqq jpg width crop smart auto webp eda ba bf ed fakealbumcov irish rover jolli rove tar kna fire made becam green place redd true cd anx https preview redd nw nexjtfaa jpg width crop smart auto webp ee da ce da mildlyinterest fire made becam green place rhym jarat true cf qh http imgur com crvur jpg wvf psbattl artwork jarat dagnummmong stop redd true qbhc https preview redd lsvmqhtjcz jpg width crop smart auto webp bfb dd ccafa cad fakealbumcov stop top today ss without jupit probabl asteroid impact artist use one vermont senat top surrog year stormtroop finish anim imgur com true cal https extern preview redd vwnj gq tvah fdc ahmc qyzjj sw fqlrwqauk jpg width crop smart auto webp fba de ac subredditsimul without jupit probabl asteroid impact artist use one vermont senat top surrog year stormtroop finish anim jawnboy realm infinit dog chooser divin normal suffer wretch choic incorrect choos wise choos choos choos true https imgur com xfgou jpg iidm psbattl artwork realm infinit dog chooser divin normal suffer wretch choic incorrect choos wise may choos may choos may choos alexkjenkin ask ye shall receiv true https imgur com ibnidfd jpg fcbep psbattl artwork ask ye shall receiv iainesmith frank alway dread part true ccpchlf http imgur com mgnbt jpg zbx psbattl artwork frank alway dread part photoplop marseysid true cp ae http imgur com yseo jpg xkabe psbattl artwork marseysid rotoramm man meal true cypk http imgur com fy wedd jpg zvuch psbattl artwork man meal glen savet snow machin someon driveway redd true ctbeo https preview redd rlt qh jpg width crop smart auto webp ac efd add abfdcc mildlyinterest snow machin someon driveway deathhamst river blind vaccin edg closer ed ac uk true dmjq https extern preview redd yr zogaoiargw cdzp ts ctktfv wzqa oi jpg width crop smart auto webp cc ed upliftingnew river blind vaccin edg closer lasagna boy plant grew gross drain found redd true cvz https preview redd nkebqkn xi jpg width crop smart auto webp ece cdd ea dbf mildlyinterest plant grew gross drain found carwer windsurf southwest imgur com true dr https extern preview redd dczwkjqpxhvvkcckzdopeleq jvt fpqpddquzaltm jpg width crop smart auto webp fe photoshopbattl windsurf southwest lazarus hindu devote gang flickr com true svv https extern preview redd av sjqoxjgu qdlfqqahsekm ro rm gle jpg width crop smart auto webp eb acdaf aa fb cf pic hindu devote gang okthatskooliguess sloucher rebellion redd true gop https preview redd iq dvuf jpg width crop smart auto webp bd ea fakealbumcov sloucher rebellion spamholderman nigeria ebola meme use spread fear virus creat white peopl ibtim co uk true gti https extern preview redd ha vrflokhzf bfmoaltsomovnyco qn ikrplid jpg width crop smart auto webp bf de bfd eead nottheonion nigeria ebola meme use spread fear virus creat white peopl sunflow thrill decay imgur com true ddf https extern preview redd heeeom kpidop djgnzvilo crnxcbotythtdbmgi jpg width crop smart auto webp bfb bc pareidolia sunflow thrill decay royalprincesoldi headbuttbutthead true ck http imgur com jpg ej kd psbattl artwork headbutt butthead jexomwtf heinz ketchunnais redd true byiy https preview redd ev iqv jpg width crop smart auto webp fc ed fec bdad ed mildlyinterest heinz ketchunnais jute siberian villag hope name chang syria get moscow attent rferl org true https extern preview redd wni xj tx bnyhhzhkvyuc ltka ku gsbmn gmo jpg width crop smart auto webp fca edad nottheonion siberian villag hope name chang syria get moscow attent epic snot rocket true csi http imgur com rzxwz jpg cnql psbattl artwork epic snot rocket kogeliz emot support turkey look airplan window imgur com true moc https extern preview redd esofnumep pz dlw ck krcektura jliiaeacxw jpg width crop smart auto webp ae fb cab df ac fdef photoshopbattl psbattl emot support turkey look airplan window psbbot avail true cokn https imgur com hkdvex jpg vsyqp psbattl artwork avail smallszac long babi carrot redd true bx https preview redd exvj jpg width crop smart auto webp bfc baaaae fe mildlyinterest long babi carrot theghostoftzvika disabl lawsuit frequent filer law meant help disabl unintend consequ economist com true https extern preview redd oyyx lnfpyhyjj uz rpn zbtihni qqglxrrzw jpg width crop smart auto webp ab bc ce usanew disabl lawsuit frequent filer law meant help disabl unintend consequ nowordofali obama perman protect one million acr public land thinkprogress org true cwguz https extern preview redd htx xol jh rwx trqo mu rquyn siq jpg width crop smart auto webp eeb de bac upliftingnew obama perman protect one million acr public land kid dive lake imgur com true https extern preview redd crfktocxr htnzkvzqogybelimvq xzcowa jpg width crop smart auto webp fa bc photoshopbattl psbattl kid dive lake raj floor kitchen crack way someth come beneath redd true ad hir https preview redd oqttpk jpg width crop smart auto webp eea fe mildlyinterest floor kitchen crack way someth come beneath id concern danica true cu http imgur com yid jpg hcoki psbattl artwork concern danica darwin icicl gather one tree redd true ccw gq https preview redd lfwaf vn jpg width crop smart auto webp ba ff bbd ec mildlyinterest icicl gather one tree jeffrey picasso left beach dock imgur com true eijow https extern preview redd icqfq qcrhjci ywxgrkconwsitydr yowqmkvbbklm jpg width crop smart auto webp fb pareidolia picasso left beach dock bleepzork play first time post pleas gentl imgur com true https extern preview redd cbm fuwv mrkda ghs wgtil zbdrw oz trzvue gif width crop smart format png dbba misleadingthumbnail play mysel first time post pleas gentl mr bonner hors redd true grr https preview redd ik qq fog jpg width crop smart auto webp cfcc fd ffa ca confus perspect hors flyinghighernow nonprofit back trump deep swampi tie opensecret org true nt https extern preview redd qiwl optgwehxctcxny gasnvoszyhvrbi qbe jpg width crop smart auto webp fd ac ead af dae usnew nonprofit back trump deep swampi tie everybodi dday redd true yia https preview redd wlrdzdl png width crop smart auto webp afbab bffd fakehistoryporn day june color smobuchin beach true ckj http imgur com fwovovo jpg ghq psbattl artwork beach voic wood former ninja warrior save choke man life wkbw com true njkf https extern preview redd bdjmm yvw rvhdd lk qvg surz ag fs jpg width crop smart auto webp fb fddcc cf de ab cb upliftingnew former ninja warrior save choke man life jaw guy pleas pizza imgur com true lqg https extern preview redd exodpupuu ynkz ae tenfbxbx xsmdsj kzftizzmk jpg width crop smart auto webp dfc photoshopbattl guy pleas pizza firecheetah menu nazi death camp restaur redd true cf kg https preview redd ts yxr mm jpg width crop smart auto webp cb fakehistoryporn menu nazi death camp restaur tent phantoka ucc open food pantri assist struggl student gazett com true https extern preview redd nqorl oouo zn tjlilrcnciwkj vat rco jpg width crop smart auto webp fc dde ed aa feb df upliftingnew ucc open food pantri assist struggl student post nottheonion anatoli dyatlov toptunov told reactor explod chernobyl redd true bxjyan https preview redd oc jpg width crop smart auto webp ec ba df aadf fakehistoryporn anatoli dyatlov toptunov told reactor explod chernobyl color elchappiea rain left pattern wood chip redd true tnga https preview redd run cg jpg width crop smart auto webp cc bc bb ea mildlyinterest rain left pattern wood chip provenz alway said small sizeh show true rju http imgur com lo bch jpg uaj psbattl artwork alway said small size show leonickl dont come insid true ly http imgur com adbv jpg api psbattl artwork come insid occamsax interest bit christian found work transcript comment imgur com true coo https extern preview redd vcwniszoibcnykktmsutmyqyasjntramcm jpg width crop smart auto webp ba eaf propagandapost interest bit christian propaganda found work transcript comment mattryd nebraska chicken thief send kfc letter apolog omaha com true https extern preview redd ez kanuczf ruor gcsw xxei cjah pna prkc jpg width crop smart auto webp cbfae de fcab nottheonion nebraska chicken thief send kfc letter apolog gracebatmonkey communiti rescu pumpkin extravaganza disadvantag kid mere hour familiesforlakec com true op jf https extern preview redd mh zz ip ihdcuakvul ftjka ocngz qxsu jpg width crop smart auto webp fd eb ee da fb upliftingnew communiti rescu pumpkin extravaganza disadvantag kid mere hour ftanuki omg buti true ch http imgur com xaxczvk jpg xte psbattl artwork omg buti potallegta man say dollar store trick famili leav houston chron com true dic https extern preview redd nkzktatfpvvddiptaq agzkdo szlypm hie jpg width crop smart auto webp fed af ad de nottheonion man say dollar store trick famili leav houston mjbott imag playboy mansion redd true ldp https preview redd vxmzs png width crop smart auto webp cdd afcec fakehistoryporn imag playboy mansion ignatz marshmallow penguin imgur com true ehdt https extern preview redd xzi lwfz ejioqwa rcuimnfqjg hmejgmk qg lw jpg width crop smart auto webp cd fd cfa cd acda de pareidolia marshmallow penguin wall nose size dog redd true ct https preview redd ewo yxr ph gif width crop smart format png bd cb confus perspect size dog fiskfisk dont panic true cc af http imgur com ie intr jpg lroym psbattl artwork panic']\n\nVector representation of sentence:\n          aa       ab        ac       acr        ad       add        ae  \\\n0  0.011699  0.01137  0.027005  0.011329  0.007622  0.004881  0.017896   \n\n         af        ag   airplan     alway      anim    apolog    artist  \\\n0  0.016819  0.006032  0.010488  0.016603  0.007908  0.009554  0.008293   \n\n    artwork       ask    assist  asteroid    attent      auto  auto webp  \\\n0  0.074503  0.007329  0.009865   0.01125  0.010022  0.188691   0.208128   \n\n      avail      ave        ba      babi      back        bb        bc  \\\n0  0.010346  0.00619  0.023738  0.007629  0.006415  0.005685  0.018242   \n\n         bd     beach     becam  beneath        bf  birthday       bit  \\\n0  0.012136  0.016902  0.009176  0.01137  0.011289  0.004315  0.009032   \n\n      blind       bob       boy        ca       cab      camp    carrot  \\\n0  0.009338  0.005183  0.003674  0.004321  0.006106  0.008707  0.011547   \n\n         cb        cc        cd        ce        cf        ch     chang  \\\n0  0.017994  0.017394  0.016559  0.011007  0.023619  0.006068  0.007097   \n\n   chernobyl  ...    thrill       tie      time  time post     today  \\\n0   0.010532  ...  0.011996  0.009422  0.005897   0.012471  0.003519   \n\n       told       top  transcript      tree     trick      true  true https  \\\n0  0.008228  0.011143    0.012064  0.007612  0.009597  0.238112    0.047012   \n\n      trump    turkey       tx        uk  upliftingnew       use   use one  \\\n0  0.005657  0.010037  0.00575  0.008197      0.029246  0.011469  0.012136   \n\n     vaccin   vermont   villag     virus     voic      wall   warrior  \\\n0  0.010289  0.011646  0.00932  0.010857  0.00505  0.003851  0.010405   \n\n        way      webp   webp ae   webp ba   webp cb   webp cc   webp cd  \\\n0  0.006619  0.208128  0.006338  0.012792  0.006396  0.012792  0.006396   \n\n    webp eb  webp fa   webp fd   webp fe     white  white peopl     width  \\\n0  0.006396  0.00619  0.012792  0.006396  0.006925     0.012136  0.216566   \n\n   width crop    window      wise   without      wood     work      xi  \\\n0    0.219378  0.008144  0.012136  0.007829  0.013624  0.00627  0.0053   \n\n         ye      year        yr  \n0  0.012471  0.004979  0.005625  \n\n[1 rows x 368 columns]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### Validation set","metadata":{"id":"yeDxVFZa5zMT"}},{"cell_type":"code","source":"from sklearn.model_selection import PredefinedSplit\n# Further split the original training set to a train and a validation set\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, train_size = 0.8, stratify = y, random_state = 2022)\n\n# Create a list where train data indices are -1 and validation data indices are 0\n# X_train2 (new training set), X_train\nsplit_index = [-1 if x in X_train.index else 0 for x in X.index]\n\n# Use the list to create PredefinedSplit\npds = PredefinedSplit(test_fold = split_index)\n","metadata":{"id":"JOSWWp1453xD","execution":{"iopub.status.busy":"2023-04-06T00:07:24.118929Z","iopub.execute_input":"2023-04-06T00:07:24.119358Z","iopub.status.idle":"2023-04-06T00:07:24.314088Z","shell.execute_reply.started":"2023-04-06T00:07:24.119313Z","shell.execute_reply":"2023-04-06T00:07:24.312703Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Built pipline\n# The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# It saves time by applying any preprocessing to both train and test data without repeating the process.\n\npipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  MLPClassifier(\n        random_state=1,\n        solver=\"adam\",\n        hidden_layer_sizes=(12, 12, 12),\n        activation=\"relu\",\n        early_stopping=True,\n        n_iter_no_change=1,\n    ))])\n\n# define parameter space to test\nparams = {\n    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n    # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n    # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n    # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n    \"tfidf__max_df\": np.arange(0.3, 0.8),\n    # max_df ignore terms that have a document frequency strictly higher than the given threshold\n    \"tfidf__min_df\": np.arange(5, 100),\n    # min_df ignore terms that have a document frequency strictly lower than the given threshold\n}\n\npipe_clf = RandomizedSearchCV(\n    pipe, params, cv=pds, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\n# here we still use X but the Randomized search model by use validation set \n# fit the pipeline\npipe_clf.fit(X, y)\n\nprint('best score {}'.format(pipe_clf.best_score_))\nprint('best score {}'.format(pipe_clf.best_params_))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"keAY9lmG5ux7","outputId":"6fdafebc-3cd3-4ba2-fd95-7f92269d5a42","execution":{"iopub.status.busy":"2023-04-06T00:07:24.315465Z","iopub.execute_input":"2023-04-06T00:07:24.315843Z","iopub.status.idle":"2023-04-06T00:08:05.638994Z","shell.execute_reply.started":"2023-04-06T00:07:24.315805Z","shell.execute_reply":"2023-04-06T00:08:05.637372Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 10 candidates, totalling 10 fits\nbest score 0.8605539695954626\nbest score {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 13, 'tfidf__max_df': 0.3}\n","output_type":"stream"}]},{"cell_type":"code","source":"# run pipe with optimized parameters\nbest_params = pipe_clf.best_params_\npipe.set_params(**best_params).fit(X, y)\npipe_pred = pipe.predict(X_test)","metadata":{"id":"YjDHpkzI5u0z","execution":{"iopub.status.busy":"2023-04-06T00:08:05.641550Z","iopub.execute_input":"2023-04-06T00:08:05.642496Z","iopub.status.idle":"2023-04-06T00:08:13.571525Z","shell.execute_reply.started":"2023-04-06T00:08:05.642427Z","shell.execute_reply":"2023-04-06T00:08:13.569710Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Use this cell to write the result in the excel sheet.\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_ts['id']\n\nsubmission['label'] = pipe.predict_proba(X_test)[:,1]\n\nsubmission.to_csv('sample_submission_walkthrough1.csv', index=False)","metadata":{"id":"oWjAUlFu5u-T","execution":{"iopub.status.busy":"2023-04-06T00:08:13.574686Z","iopub.execute_input":"2023-04-06T00:08:13.576139Z","iopub.status.idle":"2023-04-06T00:08:14.709247Z","shell.execute_reply.started":"2023-04-06T00:08:13.576054Z","shell.execute_reply":"2023-04-06T00:08:14.707519Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Built pipline\n# The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# It saves time by applying any preprocessing to both train and test data without repeating the process.\n\npipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  MLPClassifier(\n        random_state=1,\n        solver=\"adam\",\n        hidden_layer_sizes=(12, 12, 12),\n        activation=\"relu\",\n        early_stopping=True,\n        n_iter_no_change=1,\n    ))])\n\n# define parameter space to test\nparams = {\n    # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n    \"tfidf__max_df\": np.arange(0.3, 0.8),\n    # max_df ignore terms that have a document frequency strictly higher than the given threshold\n    \"tfidf__min_df\": np.arange(5, 100),\n    # min_df ignore terms that have a document frequency strictly lower than the given threshold\n}\n\npipe_clf = BayesSearchCV(\n    pipe, params, cv=pds, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\n# here we still use X but the Bayes search model by use validation set \n# fit the pipeline\npipe_clf.fit(X, y)\n\nprint('best score {}'.format(pipe_clf.best_score_))\nprint('best score {}'.format(pipe_clf.best_params_))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fRXf1qqlIQqO","outputId":"a3c6d765-5bfd-4659-9bb1-bfe6916c3ae0","execution":{"iopub.status.busy":"2023-04-06T00:08:14.710784Z","iopub.execute_input":"2023-04-06T00:08:14.711167Z","iopub.status.idle":"2023-04-06T00:12:08.227723Z","shell.execute_reply.started":"2023-04-06T00:08:14.711128Z","shell.execute_reply":"2023-04-06T00:12:08.225617Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n","output_type":"stream"},{"name":"stdout","text":"Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n","output_type":"stream"},{"name":"stdout","text":"Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n","output_type":"stream"},{"name":"stdout","text":"Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nbest score 0.8610720921284429\nbest score OrderedDict([('tfidf__max_df', 0.3), ('tfidf__min_df', 6)])\n","output_type":"stream"}]},{"cell_type":"code","source":"# run pipe with optimized parameters\nbest_params = pipe_clf.best_params_\npipe.set_params(**best_params).fit(X, y)\npipe_pred = pipe.predict(X_test)","metadata":{"id":"vY5Pjrt0IQqO","execution":{"iopub.status.busy":"2023-04-06T00:12:08.230721Z","iopub.execute_input":"2023-04-06T00:12:08.232068Z","iopub.status.idle":"2023-04-06T00:12:14.758495Z","shell.execute_reply.started":"2023-04-06T00:12:08.231985Z","shell.execute_reply":"2023-04-06T00:12:14.756407Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Use this cell to write the result in the excel sheet.\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_ts['id']\n\nsubmission['label'] = pipe.predict_proba(X_test)[:,1]\n\nsubmission.to_csv('sample_submission_walkthrough2.csv', index=False)","metadata":{"id":"SJPdHNfcIQqP","execution":{"iopub.status.busy":"2023-04-06T00:12:14.761297Z","iopub.execute_input":"2023-04-06T00:12:14.762093Z","iopub.status.idle":"2023-04-06T00:12:15.550440Z","shell.execute_reply.started":"2023-04-06T00:12:14.762007Z","shell.execute_reply":"2023-04-06T00:12:15.549189Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Built pipline\n# The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# It saves time by applying any preprocessing to both train and test data without repeating the process.\n\npipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  MLPClassifier(\n        random_state=1,\n        solver='lbfgs',\n        hidden_layer_sizes=(12, 12, 12),\n        activation=\"relu\",\n        early_stopping=True,\n        alpha=0.0001,\n        learning_rate= 'adaptive',\n    ))])\n\n# define parameter space to test\nparams = {\n    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n    # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n    # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n    # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n    \"tfidf__max_df\": np.arange(0.3, 0.8),\n    # max_df ignore terms that have a document frequency strictly higher than the given threshold\n    \"tfidf__min_df\": np.arange(5, 100),\n    # min_df ignore terms that have a document frequency strictly lower than the given threshold\n}\n\npipe_clf = RandomizedSearchCV(\n    pipe, params, cv=pds, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\n# here we still use X but the Randomized search model by use validation set \n# fit the pipeline\npipe_clf.fit(X, y)\n\nprint('best score {}'.format(pipe_clf.best_score_))\nprint('best score {}'.format(pipe_clf.best_params_))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3ECuHkAIRPH","outputId":"aef04799-03a6-43d6-e336-b61039004ef1","execution":{"iopub.status.busy":"2023-04-06T00:12:15.551902Z","iopub.execute_input":"2023-04-06T00:12:15.552271Z","iopub.status.idle":"2023-04-06T00:15:09.826116Z","shell.execute_reply.started":"2023-04-06T00:12:15.552237Z","shell.execute_reply":"2023-04-06T00:15:09.824438Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 10 candidates, totalling 10 fits\nbest score 0.8598078855371842\nbest score {'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 17, 'tfidf__max_df': 0.3}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","output_type":"stream"}]},{"cell_type":"code","source":"# run pipe with optimized parameters\nbest_params = pipe_clf.best_params_\npipe.set_params(**best_params).fit(X, y)\npipe_pred = pipe.predict(X_test)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7QhrL_HIRPJ","outputId":"3df42ae3-3833-4db6-86d5-41a98602f888","execution":{"iopub.status.busy":"2023-04-06T00:15:09.828702Z","iopub.execute_input":"2023-04-06T00:15:09.829586Z","iopub.status.idle":"2023-04-06T00:15:50.694014Z","shell.execute_reply.started":"2023-04-06T00:15:09.829519Z","shell.execute_reply":"2023-04-06T00:15:50.692271Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use this cell to write the result in the excel sheet.\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_ts['id']\n\nsubmission['label'] = pipe.predict_proba(X_test)[:,1]\n\nsubmission.to_csv('sample_submission_walkthrough4.csv', index=False)","metadata":{"id":"I646oHymIRPJ","execution":{"iopub.status.busy":"2023-04-06T00:15:50.696901Z","iopub.execute_input":"2023-04-06T00:15:50.698280Z","iopub.status.idle":"2023-04-06T00:15:51.986291Z","shell.execute_reply.started":"2023-04-06T00:15:50.698201Z","shell.execute_reply":"2023-04-06T00:15:51.984517Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Built pipline\n# The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# It saves time by applying any preprocessing to both train and test data without repeating the process.\n\npipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  XGBClassifier())])\n\n# define parameter space to test\nparams = {\n    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n    # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n    # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n    # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n    \"tfidf__max_df\": np.arange(0.3, 0.8),\n    # max_df ignore terms that have a document frequency strictly higher than the given threshold\n    \"tfidf__min_df\": np.arange(5, 100),\n    # min_df ignore terms that have a document frequency strictly lower than the given threshold\n    'my_classifier__min_child_weight': [20,40,80],\n    # min_child_weight is a minimal total of the weights in a child.\n    'my_classifier__max_depth':[50,60,70],  \n    # max_depth is a maximum depth of a tree\n    'my_classifier__gamma':[0.5, 1, 1.5, 2, 5],\n    # gamma is a minimum loss that we need it to split tree\n    'my_classifier__colsample_bytree':[0.6, 0.8, 1.0]\n}\n\npipe_clf = RandomizedSearchCV(\n    pipe, params, cv=pds, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\n# here we still use X but the Randomized search model by use validation set \n# fit the pipeline\npipe_clf.fit(X, y)\n\nprint('best score {}'.format(pipe_clf.best_score_))\nprint('best score {}'.format(pipe_clf.best_params_))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLoLccpxOE9X","outputId":"769959a6-bd78-4ee7-fb51-3eeeeb947dde","execution":{"iopub.status.busy":"2023-04-06T00:15:51.988048Z","iopub.execute_input":"2023-04-06T00:15:51.988528Z","iopub.status.idle":"2023-04-06T00:21:48.274608Z","shell.execute_reply.started":"2023-04-06T00:15:51.988481Z","shell.execute_reply":"2023-04-06T00:21:48.273467Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 10 candidates, totalling 10 fits\nbest score 0.8283922616410385\nbest score {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 32, 'tfidf__max_df': 0.3, 'my_classifier__min_child_weight': 20, 'my_classifier__max_depth': 50, 'my_classifier__gamma': 1, 'my_classifier__colsample_bytree': 1.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# run pipe with optimized parameters\nbest_params = pipe_clf.best_params_\npipe.set_params(**best_params).fit(X, y)\npipe_pred = pipe.predict(X_test)","metadata":{"id":"s0hWsJEPOE9Y","execution":{"iopub.status.busy":"2023-04-06T00:21:48.279933Z","iopub.execute_input":"2023-04-06T00:21:48.282224Z","iopub.status.idle":"2023-04-06T00:22:52.064075Z","shell.execute_reply.started":"2023-04-06T00:21:48.282166Z","shell.execute_reply":"2023-04-06T00:22:52.063034Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Use this cell to write the result in the excel sheet.\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_ts['id']\n\nsubmission['label'] = pipe.predict_proba(X_test)[:,1]\n\nsubmission.to_csv('sample_submission_walkthrough5.csv', index=False)","metadata":{"id":"EYZ7p8oxOE9Z","execution":{"iopub.status.busy":"2023-04-06T00:22:52.065538Z","iopub.execute_input":"2023-04-06T00:22:52.069786Z","iopub.status.idle":"2023-04-06T00:22:53.683311Z","shell.execute_reply.started":"2023-04-06T00:22:52.069729Z","shell.execute_reply":"2023-04-06T00:22:53.681447Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# # Built pipline\n# # The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# # It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# # It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# # It saves time by applying any preprocessing to both train and test data without repeating the process.\n\n# pipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  XGBClassifier())])\n\n# # define parameter space to test\n# params = {\n#     \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n#     # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n#     # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n#     # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n#     \"tfidf__max_df\": np.arange(0.3, 0.8),\n#     # max_df ignore terms that have a document frequency strictly higher than the given threshold\n#     \"tfidf__min_df\": np.arange(5, 100),\n#     # min_df ignore terms that have a document frequency strictly lower than the given threshold\n#     'my_classifier__min_child_weight': [20,40,80],\n#     # min_child_weight is a minimal total of the weights in a child.\n#     'my_classifier__max_depth':[50,60,70],  \n#     # max_depth is a maximum depth of a tree\n#     'my_classifier__gamma':[0.5, 1, 1.5, 2, 5],\n#     # gamma is a minimum loss that we need it to split tree\n#     'my_classifier__colsample_bytree':[0.6, 0.8, 1.0]\n# }\n\n# pipe_clf = GridSearchCV(\n#     pipe, params, cv=pds, verbose=1, n_jobs=2, \n#     scoring='roc_auc')\n\n# # here we still use X but the Randomized search model by use validation set \n# # fit the pipeline\n# pipe_clf.fit(X, y)\n\n# print('best score {}'.format(pipe_clf.best_score_))\n# print('best score {}'.format(pipe_clf.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:22:53.687912Z","iopub.execute_input":"2023-04-06T00:22:53.688640Z","iopub.status.idle":"2023-04-06T00:22:53.699458Z","shell.execute_reply.started":"2023-04-06T00:22:53.688580Z","shell.execute_reply":"2023-04-06T00:22:53.698111Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# # run pipe with optimized parameters\n# best_params = pipe_clf.best_params_\n# pipe.set_params(**best_params).fit(X, y)\n# pipe_pred = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:22:53.700945Z","iopub.execute_input":"2023-04-06T00:22:53.701553Z","iopub.status.idle":"2023-04-06T00:22:53.712699Z","shell.execute_reply.started":"2023-04-06T00:22:53.701515Z","shell.execute_reply":"2023-04-06T00:22:53.711705Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# # Use this cell to write the result in the excel sheet.\n# submission = pd.DataFrame()\n\n# submission['id'] = data_ts['id']\n\n# submission['label'] = pipe.predict_proba(X_test)[:,1]\n\n# submission.to_csv('sample_submission_walkthrough10.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:22:53.714779Z","iopub.execute_input":"2023-04-06T00:22:53.715705Z","iopub.status.idle":"2023-04-06T00:22:53.728611Z","shell.execute_reply.started":"2023-04-06T00:22:53.715653Z","shell.execute_reply":"2023-04-06T00:22:53.727364Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# # Built pipline\n# # The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# # It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# # It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# # It saves time by applying any preprocessing to both train and test data without repeating the process.\n\n# pipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  XGBClassifier())])\n\n# # define parameter space to test\n# params = {\n#     \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n#     # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n#     # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n#     # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n#     \"tfidf__max_df\": np.arange(0.3, 0.8),\n#     # max_df ignore terms that have a document frequency strictly higher than the given threshold\n#     \"tfidf__min_df\": np.arange(5, 100),\n#     # min_df ignore terms that have a document frequency strictly lower than the given threshold\n#     'my_classifier__min_child_weight': [20,40,80],\n#     # min_child_weight is a minimal total of the weights in a child.\n#     'my_classifier__max_depth':[50,60,70],  \n#     # max_depth is a maximum depth of a tree\n#     'my_classifier__gamma':[0.5, 1, 1.5, 2, 5],\n#     # gamma is a minimum loss that we need it to split tree\n#     'my_classifier__colsample_bytree':[0.6, 0.8, 1.0]\n# }\n\n# pipe_clf = BayesSearchCV(\n#     pipe, params, cv=pds, verbose=1, n_jobs=2, \n#     scoring='roc_auc')\n\n# # here we still use X but the Randomized search model by use validation set \n# # fit the pipeline\n# pipe_clf.fit(X, y)\n\n# print('best score {}'.format(pipe_clf.best_score_))\n# print('best score {}'.format(pipe_clf.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:22:53.730432Z","iopub.execute_input":"2023-04-06T00:22:53.730910Z","iopub.status.idle":"2023-04-06T00:22:53.741632Z","shell.execute_reply.started":"2023-04-06T00:22:53.730830Z","shell.execute_reply":"2023-04-06T00:22:53.740245Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# # run pipe with optimized parameters\n# best_params = pipe_clf.best_params_\n# pipe.set_params(**best_params).fit(X, y)\n# pipe_pred = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:22:53.742971Z","iopub.execute_input":"2023-04-06T00:22:53.743415Z","iopub.status.idle":"2023-04-06T00:22:53.761757Z","shell.execute_reply.started":"2023-04-06T00:22:53.743377Z","shell.execute_reply":"2023-04-06T00:22:53.760446Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# # Use this cell to write the result in the excel sheet.\n# submission = pd.DataFrame()\n\n# submission['id'] = data_ts['id']\n\n# submission['label'] = pipe.predict_proba(X_test)[:,1]\n\n# submission.to_csv('sample_submission_walkthrough11.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:22:53.763527Z","iopub.execute_input":"2023-04-06T00:22:53.764290Z","iopub.status.idle":"2023-04-06T00:22:53.780588Z","shell.execute_reply.started":"2023-04-06T00:22:53.764250Z","shell.execute_reply":"2023-04-06T00:22:53.779014Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# # Built pipline\n# # The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# # It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# # It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# # It saves time by applying any preprocessing to both train and test data without repeating the process.\n\n# pipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  LogisticRegression())])\n\n# # define parameter space to test\n# params = {\n#     \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n#     # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n#     # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n#     # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n#     \"tfidf__max_df\": np.arange(0.3, 0.8),\n#     # max_df ignore terms that have a document frequency strictly higher than the given threshold\n#     \"tfidf__min_df\": np.arange(5, 100),\n#     # min_df ignore terms that have a document frequency strictly lower than the given threshold\n#     'my_classifier__C': [100],\n#     # C controls the penality strength \n#     # my_classifier__C points to my_classifier->C\n#     'my_classifier__max_iter':[100], \n#     # max_iter is a maximum number of iterations\n#     # my_classifier__max_iter points to my_classifier-> max_iter\n#     'my_classifier__tol':[1e-4,1e-5,1e-3]\n#     # tol is a tolerance for stopping\n#     # my_classifier__tol points to my_classifier-> tol\n\n# }\n\n# pipe_clf = GridSearchCV(\n#     pipe, params, cv=pds, verbose=1, n_jobs=2, \n#     scoring='roc_auc')\n\n# # here we still use X but the Grid search model by use validation set \n# # fit the pipeline\n# pipe_clf.fit(X, y)\n\n# print('best score {}'.format(pipe_clf.best_score_))\n# print('best score {}'.format(pipe_clf.best_params_))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDqm3y0OT_s6","outputId":"5c7b5125-23ad-4eb3-97d8-5f6137d7cc1f","execution":{"iopub.status.busy":"2023-04-06T00:22:53.782270Z","iopub.execute_input":"2023-04-06T00:22:53.782662Z","iopub.status.idle":"2023-04-06T00:22:53.794932Z","shell.execute_reply.started":"2023-04-06T00:22:53.782625Z","shell.execute_reply":"2023-04-06T00:22:53.793522Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# # run pipe with optimized parameters\n# best_params = pipe_clf.best_params_\n# pipe.set_params(**best_params).fit(X, y)\n# pipe_pred = pipe.predict(X_test)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ku95BSC2T_s7","outputId":"f8c0eef9-54ec-4e99-f244-9f2cb5cba840","execution":{"iopub.status.busy":"2023-04-06T00:22:53.796659Z","iopub.execute_input":"2023-04-06T00:22:53.797076Z","iopub.status.idle":"2023-04-06T00:22:53.811470Z","shell.execute_reply.started":"2023-04-06T00:22:53.797037Z","shell.execute_reply":"2023-04-06T00:22:53.810128Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# # Use this cell to write the result in the excel sheet.\n# submission = pd.DataFrame()\n\n# submission['id'] = data_ts['id']\n\n# submission['label'] = pipe.predict_proba(X_test)[:,1]\n\n# submission.to_csv('sample_submission_walkthrough6.csv', index=False)","metadata":{"id":"-RIBqm_iT_s7","execution":{"iopub.status.busy":"2023-04-06T00:22:53.813524Z","iopub.execute_input":"2023-04-06T00:22:53.813947Z","iopub.status.idle":"2023-04-06T00:22:53.829359Z","shell.execute_reply.started":"2023-04-06T00:22:53.813892Z","shell.execute_reply":"2023-04-06T00:22:53.827945Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Built pipline\n# The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# It saves time by applying any preprocessing to both train and test data without repeating the process.\n\npipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  LogisticRegression())])\n\n# define parameter space to test\nparams = {\n    \"tfidf__max_df\": np.arange(0.3, 0.8),\n    # max_df ignore terms that have a document frequency strictly higher than the given threshold\n    \"tfidf__min_df\": np.arange(5, 100),\n    # min_df ignore terms that have a document frequency strictly lower than the given threshold\n    'my_classifier__C': [100,200,300],\n    # C controls the penality strength \n    # my_classifier__C points to my_classifier->C\n    'my_classifier__max_iter':[100 ,200, 300], \n    # max_iter is a maximum number of iterations\n    # my_classifier__max_iter points to my_classifier-> max_iter\n    'my_classifier__tol':[1e-4,1e-5,1e-3]\n    # tol is a tolerance for stopping\n    # my_classifier__tol points to my_classifier-> tol\n\n}\n\npipe_clf = BayesSearchCV(\n    pipe, params, cv=pds, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\n# here we still use X but the Randomized search model by use validation set \n# fit the pipeline\npipe_clf.fit(X, y)\n\nprint('best score {}'.format(pipe_clf.best_score_))\nprint('best score {}'.format(pipe_clf.best_params_))","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"lj2AnMKTT_s8","outputId":"d812d4b7-8835-4640-9d49-4164911402d6","execution":{"iopub.status.busy":"2023-04-06T00:22:53.831221Z","iopub.execute_input":"2023-04-06T00:22:53.831576Z","iopub.status.idle":"2023-04-06T00:26:50.561379Z","shell.execute_reply.started":"2023-04-06T00:22:53.831543Z","shell.execute_reply":"2023-04-06T00:26:50.560151Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"Fitting 1 folds for each of 1 candidates, totalling 1 fits\nbest score 0.8497117748069006\nbest score OrderedDict([('my_classifier__C', 100), ('my_classifier__max_iter', 300), ('my_classifier__tol', 0.001), ('tfidf__max_df', 0.3), ('tfidf__min_df', 22)])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"}]},{"cell_type":"code","source":"# run pipe with optimized parameters\nbest_params = pipe_clf.best_params_\npipe.set_params(**best_params).fit(X, y)\npipe_pred = pipe.predict(X_test)","metadata":{"colab":{"background_save":true},"id":"Q7UTOTz6T_s8","outputId":"81ebb09f-cb06-4f4c-d3ab-3f05f29e5cb0","execution":{"iopub.status.busy":"2023-04-06T00:26:50.563143Z","iopub.execute_input":"2023-04-06T00:26:50.563645Z","iopub.status.idle":"2023-04-06T00:26:54.505350Z","shell.execute_reply.started":"2023-04-06T00:26:50.563595Z","shell.execute_reply":"2023-04-06T00:26:54.504256Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use this cell to write the result in the excel sheet.\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_ts['id']\n\nsubmission['label'] = pipe.predict_proba(X_test)[:,1]\n\nsubmission.to_csv('sample_submission_walkthrough7.csv', index=False)","metadata":{"colab":{"background_save":true},"id":"EOOW9Pb4T_s8","execution":{"iopub.status.busy":"2023-04-06T00:26:54.506791Z","iopub.execute_input":"2023-04-06T00:26:54.507135Z","iopub.status.idle":"2023-04-06T00:26:55.101786Z","shell.execute_reply.started":"2023-04-06T00:26:54.507102Z","shell.execute_reply":"2023-04-06T00:26:55.100103Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Built pipline\n# The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# It saves time by applying any preprocessing to both train and test data without repeating the process.\n\npipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  RandomForestClassifier())])\n\n# define parameter space to test\nparams = {\n    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n    # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n    # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n    # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n    \"tfidf__max_df\": np.arange(0.3, 0.8),\n    # max_df ignore terms that have a document frequency strictly higher than the given threshold\n    \"tfidf__min_df\": np.arange(5, 100),\n    # min_df ignore terms that have a document frequency strictly lower than the given threshold\n    'my_classifier__n_estimators': [170,200,250],\n    # n_estimators is a number of trees \n    # my_classifier__n_estimators points to my_classifier->n_estimators\n    'my_classifier__max_depth':[70,80,90],   \n    # max_depth is a maximum depth of the tree\n    # my_classifier__max_depth points to my_classifier-> max_depth\n    'my_classifier__max_features':[10,20,30]\n    # max_features is a maximum number of features\n    # my_classifier__max_features points to my_classifier-> max_features\n\n}\n\npipe_clf = RandomizedSearchCV(\n    pipe, params, cv=pds, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\n# here we still use X but the Randomized search model by use validation set \n# fit the pipeline\npipe_clf.fit(X, y)\n\nprint('best score {}'.format(pipe_clf.best_score_))\nprint('best score {}'.format(pipe_clf.best_params_))","metadata":{"id":"iZXs7QP0oml1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"080ceb1b-b72d-4bb3-bcf7-bdd142a2f7da","execution":{"iopub.status.busy":"2023-04-06T00:26:55.104138Z","iopub.execute_input":"2023-04-06T00:26:55.104874Z","iopub.status.idle":"2023-04-06T00:30:17.372354Z","shell.execute_reply.started":"2023-04-06T00:26:55.104823Z","shell.execute_reply":"2023-04-06T00:30:17.371117Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 10 candidates, totalling 10 fits\nbest score 0.8370142953219597\nbest score {'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 17, 'tfidf__max_df': 0.3, 'my_classifier__n_estimators': 170, 'my_classifier__max_features': 20, 'my_classifier__max_depth': 90}\n","output_type":"stream"}]},{"cell_type":"code","source":"# run pipe with optimized parameters\nbest_params = pipe_clf.best_params_\npipe.set_params(**best_params).fit(X, y)\npipe_pred = pipe.predict(X_test)","metadata":{"id":"AKVv_Af9oml2","execution":{"iopub.status.busy":"2023-04-06T00:30:17.375921Z","iopub.execute_input":"2023-04-06T00:30:17.376688Z","iopub.status.idle":"2023-04-06T00:30:49.122377Z","shell.execute_reply.started":"2023-04-06T00:30:17.376646Z","shell.execute_reply":"2023-04-06T00:30:49.121252Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Use this cell to write the result in the excel sheet.\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_ts['id']\n\nsubmission['label'] = pipe.predict_proba(X_test)[:,1]\n\nsubmission.to_csv('sample_submission_walkthrough8.csv', index=False)","metadata":{"id":"_3rfY6jZoml2","execution":{"iopub.status.busy":"2023-04-06T00:30:49.123767Z","iopub.execute_input":"2023-04-06T00:30:49.124104Z","iopub.status.idle":"2023-04-06T00:30:53.909169Z","shell.execute_reply.started":"2023-04-06T00:30:49.124072Z","shell.execute_reply":"2023-04-06T00:30:53.908121Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# pipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", norm=\"l2\")), (\"my_classifier\",  RandomForestClassifier())])\n\n# # define parameter space to test\n# params = {\n#     \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n#     # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n#     # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n#     # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n#     \"tfidf__max_df\": np.arange(0.3, 0.8),\n#     # max_df ignore terms that have a document frequency strictly higher than the given threshold\n#     \"tfidf__min_df\": np.arange(5, 100),\n#     # min_df ignore terms that have a document frequency strictly lower than the given threshold\n#     'my_classifier__n_estimators': [170,200,250],\n#     # n_estimators is a number of trees \n#     # my_classifier__n_estimators points to my_classifier->n_estimators\n#     'my_classifier__max_depth':[70,80,90],   \n#     # max_depth is a maximum depth of the tree\n#     # my_classifier__max_depth points to my_classifier-> max_depth\n#     'my_classifier__max_features':[10,20,30]\n#     # max_features is a maximum number of features\n#     # my_classifier__max_features points to my_classifier-> max_features\n\n# }\n\n# pipe_clf = GridSearchCV(\n#     pipe, params, cv=pds, verbose=1, n_jobs=2, \n#     scoring='roc_auc')\n\n# # here we still use X but the Randomized search model by use validation set \n# # fit the pipeline\n# pipe_clf.fit(X, y)\n\n# print('best score {}'.format(pipe_clf.best_score_))\n# print('best score {}'.format(pipe_clf.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:30:53.910873Z","iopub.execute_input":"2023-04-06T00:30:53.911717Z","iopub.status.idle":"2023-04-06T00:30:53.917671Z","shell.execute_reply.started":"2023-04-06T00:30:53.911675Z","shell.execute_reply":"2023-04-06T00:30:53.916387Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# # run pipe with optimized parameters\n# best_params = pipe_clf.best_params_\n# pipe.set_params(**best_params).fit(X, y)\n# pipe_pred = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:30:53.919144Z","iopub.execute_input":"2023-04-06T00:30:53.919508Z","iopub.status.idle":"2023-04-06T00:30:53.933132Z","shell.execute_reply.started":"2023-04-06T00:30:53.919475Z","shell.execute_reply":"2023-04-06T00:30:53.931836Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# # Use this cell to write the result in the excel sheet.\n# submission = pd.DataFrame()\n\n# submission['id'] = data_ts['id']\n\n# submission['label'] = pipe.predict_proba(X_test)[:,1]\n\n# submission.to_csv('sample_submission_walkthrough9.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:30:53.934739Z","iopub.execute_input":"2023-04-06T00:30:53.935110Z","iopub.status.idle":"2023-04-06T00:30:53.943788Z","shell.execute_reply.started":"2023-04-06T00:30:53.935075Z","shell.execute_reply":"2023-04-06T00:30:53.942945Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Built pipline\n# The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# It saves time by applying any preprocessing to both train and test data without repeating the process.\n\npipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char\", norm=\"l2\")), (\"my_classifier\",  XGBClassifier())])\n\n# define parameter space to test\nparams = {\n    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n    # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n    # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n    # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n    \"tfidf__max_df\": np.arange(0.3, 0.8),\n    # max_df ignore terms that have a document frequency strictly higher than the given threshold\n    \"tfidf__min_df\": np.arange(5, 100),\n    # min_df ignore terms that have a document frequency strictly lower than the given threshold\n    'my_classifier__min_child_weight': [20,40,80],\n    # min_child_weight is a minimal total of the weights in a child.\n    'my_classifier__max_depth':[50,60,70],  \n    # max_depth is a maximum depth of a tree\n    'my_classifier__gamma':[0.5, 1, 1.5, 2, 5],\n    # gamma is a minimum loss that we need it to split tree\n    'my_classifier__colsample_bytree':[0.6, 0.8, 1.0]\n}\n\npipe_clf = RandomizedSearchCV(\n    pipe, params, cv=pds, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\n# here we still use X but the Randomized search model by use validation set \n# fit the pipeline\npipe_clf.fit(X, y)\n\nprint('best score {}'.format(pipe_clf.best_score_))\nprint('best score {}'.format(pipe_clf.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:30:53.945094Z","iopub.execute_input":"2023-04-06T00:30:53.945606Z","iopub.status.idle":"2023-04-06T01:40:43.924752Z","shell.execute_reply.started":"2023-04-06T00:30:53.945570Z","shell.execute_reply":"2023-04-06T01:40:43.923515Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 10 candidates, totalling 10 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"best score 0.8243818310497056\nbest score {'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 59, 'tfidf__max_df': 0.3, 'my_classifier__min_child_weight': 20, 'my_classifier__max_depth': 70, 'my_classifier__gamma': 2, 'my_classifier__colsample_bytree': 1.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# run pipe with optimized parameters\nbest_params = pipe_clf.best_params_\npipe.set_params(**best_params).fit(X, y)\npipe_pred = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T01:40:43.929080Z","iopub.execute_input":"2023-04-06T01:40:43.929452Z","iopub.status.idle":"2023-04-06T01:55:37.037224Z","shell.execute_reply.started":"2023-04-06T01:40:43.929416Z","shell.execute_reply":"2023-04-06T01:55:37.036085Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# Use this cell to write the result in the excel sheet.\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_ts['id']\n\nsubmission['label'] = pipe.predict_proba(X_test)[:,1]\n\nsubmission.to_csv('sample_submission_walkthrough12.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T01:55:37.039323Z","iopub.execute_input":"2023-04-06T01:55:37.040100Z","iopub.status.idle":"2023-04-06T01:55:41.186301Z","shell.execute_reply.started":"2023-04-06T01:55:37.040057Z","shell.execute_reply":"2023-04-06T01:55:41.185165Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# # Built pipline\n# # The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# # It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# # It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# # It saves time by applying any preprocessing to both train and test data without repeating the process.\n\n# pipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char\", norm=\"l2\")), (\"my_classifier\",  XGBClassifier())])\n\n# # define parameter space to test\n# params = {\n#     \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n#     # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n#     # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n#     # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n#     \"tfidf__max_df\": np.arange(0.3, 0.8),\n#     # max_df ignore terms that have a document frequency strictly higher than the given threshold\n#     \"tfidf__min_df\": np.arange(5, 100),\n#     # min_df ignore terms that have a document frequency strictly lower than the given threshold\n#     'my_classifier__min_child_weight': [20,40,80],\n#     # min_child_weight is a minimal total of the weights in a child.\n#     'my_classifier__max_depth':[50,60,70],  \n#     # max_depth is a maximum depth of a tree\n#     'my_classifier__gamma':[0.5, 1, 1.5, 2, 5],\n#     # gamma is a minimum loss that we need it to split tree\n#     'my_classifier__colsample_bytree':[0.6, 0.8, 1.0]\n# }\n\n# pipe_clf = GridSearchCV(\n#     pipe, params, cv=pds, verbose=1, n_jobs=2, \n#     scoring='roc_auc')\n\n# # here we still use X but the Randomized search model by use validation set \n# # fit the pipeline\n# pipe_clf.fit(X, y)\n\n# print('best score {}'.format(pipe_clf.best_score_))\n# print('best score {}'.format(pipe_clf.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-04-06T01:55:41.188055Z","iopub.execute_input":"2023-04-06T01:55:41.188733Z","iopub.status.idle":"2023-04-06T01:55:41.194338Z","shell.execute_reply.started":"2023-04-06T01:55:41.188695Z","shell.execute_reply":"2023-04-06T01:55:41.193420Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# # run pipe with optimized parameters\n# best_params = pipe_clf.best_params_\n# pipe.set_params(**best_params).fit(X, y)\n# pipe_pred = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T01:55:41.195825Z","iopub.execute_input":"2023-04-06T01:55:41.196538Z","iopub.status.idle":"2023-04-06T01:55:41.209507Z","shell.execute_reply.started":"2023-04-06T01:55:41.196501Z","shell.execute_reply":"2023-04-06T01:55:41.208594Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# # Use this cell to write the result in the excel sheet.\n# submission = pd.DataFrame()\n\n# submission['id'] = data_ts['id']\n\n# submission['label'] = pipe_clf.predict_proba(X_test)[:,1]\n\n# submission.to_csv('sample_submission_walkthrough13.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T01:55:41.211398Z","iopub.execute_input":"2023-04-06T01:55:41.212118Z","iopub.status.idle":"2023-04-06T01:55:41.226492Z","shell.execute_reply.started":"2023-04-06T01:55:41.212078Z","shell.execute_reply":"2023-04-06T01:55:41.225170Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# # Built pipline\n# # The pipeline's goal is to combine numerous processes that can be cross-validated while modifying various parameters.\n# # It does this by allowing set parameters for each step using their names and parameter names separated by a \"__\"\n# # It takes steps as a prameter that contain all the preprocessing, vectorization, and normalization that I need.\n# # It saves time by applying any preprocessing to both train and test data without repeating the process.\n\n# pipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char\", norm=\"l2\")), (\"my_classifier\",  XGBClassifier())])\n\n# # define parameter space to test\n# params = {\n#     \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n#     # tfidf__ngram_range points to TfidfVectorizer -> ngram_range\n#     # ngram_range Two words have a higher correlation than the threshold and frequently appear together.\n#     # ngram_range=(a,b)-> a is the minimum and b is the maximum size of ngrams\n#     \"tfidf__max_df\": np.arange(0.3, 0.8),\n#     # max_df ignore terms that have a document frequency strictly higher than the given threshold\n#     \"tfidf__min_df\": np.arange(5, 100),\n#     # min_df ignore terms that have a document frequency strictly lower than the given threshold\n#     'my_classifier__min_child_weight': [20,40,80],\n#     # min_child_weight is a minimal total of the weights in a child.\n#     'my_classifier__max_depth':[50,60,70],  \n#     # max_depth is a maximum depth of a tree\n#     'my_classifier__gamma':[0.5, 1, 1.5, 2, 5],\n#     # gamma is a minimum loss that we need it to split tree\n#     'my_classifier__colsample_bytree':[0.6, 0.8, 1.0]\n# }\n\n# pipe_clf = BayesSearchCV(\n#     pipe, params, cv=pds, verbose=1, n_jobs=2, \n#     scoring='roc_auc')\n\n# # here we still use X but the Randomized search model by use validation set \n# # fit the pipeline\n# pipe_clf.fit(X, y)\n\n# print('best score {}'.format(pipe_clf.best_score_))\n# print('best score {}'.format(pipe_clf.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-04-06T01:55:41.228702Z","iopub.execute_input":"2023-04-06T01:55:41.229245Z","iopub.status.idle":"2023-04-06T01:55:41.243121Z","shell.execute_reply.started":"2023-04-06T01:55:41.229205Z","shell.execute_reply":"2023-04-06T01:55:41.241549Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# # run pipe with optimized parameters\n# best_params = pipe_clf.best_params_\n# pipe.set_params(**best_params).fit(X, y)\n# pipe_pred = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T01:55:41.244858Z","iopub.execute_input":"2023-04-06T01:55:41.246331Z","iopub.status.idle":"2023-04-06T01:55:41.258780Z","shell.execute_reply.started":"2023-04-06T01:55:41.246259Z","shell.execute_reply":"2023-04-06T01:55:41.257444Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# # Use this cell to write the result in the excel sheet.\n# submission = pd.DataFrame()\n\n# submission['id'] = data_ts['id']\n\n# submission['label'] = pipe.predict_proba(X_test)[:,1]\n\n# submission.to_csv('sample_submission_walkthrough14.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T01:55:41.260578Z","iopub.execute_input":"2023-04-06T01:55:41.261063Z","iopub.status.idle":"2023-04-06T01:55:41.276341Z","shell.execute_reply.started":"2023-04-06T01:55:41.260994Z","shell.execute_reply":"2023-04-06T01:55:41.275117Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"🌈 What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\nA character n-gram is a set of co-occurring characters within a given window. It’s very similar to word n-grams, only that the window size is at the character level. A bag of character n-grams in the fastText case means a word is represented by a sum of its character n-grams1.\nWord n-grams are sequences of n words that are used as features in natural language processing tasks such as text classification and sentiment analysis2.\nCharacter n-grams tend to suffer more from the OOV (out-of-vocabulary) issue because there are many more possible character combinations than word combinations\n\n🌈 What is the difference between stop word removal and stemming? Are these techniques language-dependent?\nStop word removal and stemming are two common techniques used in natural language processing (NLP) to preprocess text data before applying machine learning algorithms. Stop word removal is the process of removing commonly used words such as “the”, “a”, “an”, “in”, “on”, etc., which do not add much meaning to the text and can be safely removed without affecting the overall meaning of the text. Stemming, on the other hand, is the process of reducing words to their base or root form by removing suffixes and prefixes. For example, “running” and “runner” would both be reduced to “run” using stemming.\nStop word removal and stemming are not language-dependent techniques. However, their effectiveness may vary depending on the language being used. For example, stop word removal may be more effective in English than in Chinese because English has a larger number of stop words. Similarly, stemming may be more effective in languages with more regular inflectional morphology such as Spanish than in languages with more irregular morphology such as English.\n\n🌈 Is tokenization techniques language dependent? Why?\nTokenization is a common technique used in natural language processing (NLP) to preprocess text data before applying machine learning algorithms. It involves breaking down a piece of text into smaller units called tokens. These tokens can be words, phrases, or even individual characters.\nTokenization techniques are not language-dependent. However, the effectiveness of tokenization may vary depending on the language being used. For example, tokenization may be more challenging in languages such as Chinese and Japanese because these languages do not use spaces between words\n\n🌈 What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\nCountVectorizer and TfidfVectorizer are both techniques used to convert text data into numerical data that can be used in machine learning algorithms. CountVectorizer counts the number of times each word appears in a document and creates a matrix of word counts. TfidfVectorizer, on the other hand, assigns a score to each word based on its frequency in a document and across all documents. This score is called the term frequency-inverse document frequency (tf-idf) score.\nTfidfVectorizer is generally considered better than CountVectorizer because it provides more information about the importance of each word. CountVectorizer only focuses on the frequency of words present in the corpus. TfidfVectorizer provides information about how important each word is to a particular document and across all documents.\nIt would not be feasible to use all possible n-grams because it would result in a very large number of features, which could lead to overfitting. Instead, you should select n-grams based on their relevance to your specific problem. For example, if you are working with text data that contains many technical terms, you may want to include more n-grams that contain those terms","metadata":{}}]}